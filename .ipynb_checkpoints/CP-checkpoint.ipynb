{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c60a5638",
   "metadata": {},
   "source": [
    "## Implementation of $RL^2$: Fast Reinforcement Learning via Slow Reinforcement Learning\n",
    "The goal of this implementation is to use an RNN agent to be able to learn in itself a Reinforcement Learning algorithm that is able to perfect the exploration and exploitation dilemma of the Multi-Armed Bandit context (Meta-RL). The algorithm is tested in multiple environments and is tested for efficacy with other state of the art agents.\n",
    "\n",
    "1. The implemenation uses a Recurrent Neural Network (RNN) which is a Gated Recurrent Unit (GRU) being fed the last action, reward and timestep.\n",
    "2. The policy is optimized using the basic REINFORCE or Vanilla Policy Gradient.\n",
    "3. Training is being performed for 20,000 tasks(environments) where each task is picked from a random and uniform distribution of means and standard deviations.!\n",
    "\n",
    "Following image shows the visualization of our implementation.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "beca1043",
   "metadata": {},
   "source": [
    "![Meta RL Implementation](Implementation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4ec03d1-a0fe-4691-adae-ba36f0aece17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30b095bb",
   "metadata": {},
   "source": [
    "#### The classes below are the environments that are used to test the efficacy of our model and the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f56baef4-7df3-406d-a090-a68a36628136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmedBanditsEnv():\n",
    "    def __init__(self, mean, stddev):\n",
    "        assert len(mean.shape) == 2 \n",
    "        assert len(stddev.shape) == 2\n",
    "        self.num_actions = mean.shape[1] \n",
    "        self.num_experiments = mean.shape[0]\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "        \n",
    "    def step(self, action):\n",
    "    \n",
    "        sampled_means = self.mean[np.arange(self.num_experiments),action]\n",
    "        sampled_stddevs = self.stddev[np.arange(self.num_experiments),action]\n",
    "        \n",
    "        reward = np.random.normal(loc=sampled_means, scale=sampled_stddevs, size=(1,self.num_experiments))\n",
    "        \n",
    "        observation, done, info = 0, False, dict()\n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return 0\n",
    "    \n",
    "    def reset_env(self):\n",
    "        self.mean = np.random.normal(size = (1, self.num_actions))\n",
    "        self.stddev = np.ones((1, self.num_actions))\n",
    "        \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np.random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "229ef643-1706-4145-b20d-f4b2c8cd37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmBanditBernoulli():\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "\n",
    "    def reset_env(self):\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "\n",
    "    def reset(self):\n",
    "        return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = np.random.binomial(n = 1, p = self.probs[action], size=1)[0]\n",
    "        observation, done, info = 0, False, dict()\n",
    "        return observation, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3850d6d-f150-4910-aa4e-55872c45386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependentArms():\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "\n",
    "    def reset_env(self):\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 1 if random.uniform(0,1) <= self.probs[action] else 0\n",
    "        observation, done, info = 0, False, dict()\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d77ae23c",
   "metadata": {},
   "source": [
    "#### Following classes define our RNN Agent. The agent is a neural network with the input layer a linear layer with an input size of 3 as it takes as input the last action, last reward and timestep a tuple $<a,r,t>$  and the second layer is a GRU with a hidden size of 48, and 2 hidden layers, the output layer is again a linear layer with an ouput size of the number of bandits or actions to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "385c29cb-f450-4132-9b9e-5968d9bc75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAgent(nn.Module):\n",
    "    def __init__(self, hiddenSize, layers,  outputSize, inputSize = 3, timesteps = 1):\n",
    "        super(RNNAgent, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.layers = layers\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "\n",
    "        self.layer1 = nn.Linear(inputSize, hiddenSize)\n",
    "        self.layer2 = nn.GRU(hiddenSize, hiddenSize, layers)\n",
    "        self.layer3 = nn.Linear(hiddenSize, outputSize)\n",
    "\n",
    "        self.hiddenLayerReset()\n",
    "    def name():\n",
    "        return 'rnn'\n",
    "\n",
    "    def hiddenLayerReset(self):\n",
    "        self.hiddenLayer = self.hidden()\n",
    "    \n",
    "    def hidden(self):\n",
    "        layers = self.layers\n",
    "        hiddenSize = self.hiddenSize\n",
    "        timesteps   = self.timesteps\n",
    "\n",
    "        return torch.randn(layers, timesteps, hiddenSize)\n",
    "\n",
    "    def forward(self, phi):\n",
    "\n",
    "        inputs = torch.relu(self.layer1(phi))\n",
    "        outputs, self.hiddenLayer = self.layer2(inputs, self.hiddenLayer)\n",
    "        logits = torch.relu(self.layer3(outputs))\n",
    "        action_probs = F.softmax(logits, dim = 2).view(-1)\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a43b794e",
   "metadata": {},
   "source": [
    "#### The input to the RNN is a tuple that of $<s,a,r,t>$ and is represented as $\\phi$ Since for the armed bandit is stateless we'll ignore the state for now and that $\\phi$ is a tuple of the values $<a,r,t>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a2c43f8-527d-4945-a9c0-fa04d454cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createphi(state, action, reward, t):\n",
    "    phi = [action, reward, t]\n",
    "    phi = np.reshape(phi, (1,1,3)).astype(np.float32)\n",
    "    \n",
    "    return torch.tensor(phi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a4a150c",
   "metadata": {},
   "source": [
    "#### This is our slow policy optimization algorithm which is a simple REINFORCE policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d02c93c9-826f-4d48-9ea2-1f6d1215eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_RL(optimizer, action_probs, rewards, gamma, action):\n",
    "    #We are using REINFORCE policy gradient here\n",
    "    Gt = 0\n",
    "    discountedRewards = []\n",
    "    policyLoss = []\n",
    "    \n",
    "    for i in reversed(rewards):\n",
    "        Gt = i + gamma * Gt\n",
    "        discountedRewards.insert(0, Gt)\n",
    "    discountedRewards = torch.tensor(discountedRewards)\n",
    "\n",
    "    for action_prob, r in zip(action_probs, discountedRewards):\n",
    "        policyLoss.append(-action_prob * r)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policyLoss = torch.cat(policyLoss).sum()\n",
    "    policyLoss.backward(retain_graph=True)\n",
    "    optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9217bae4",
   "metadata": {},
   "source": [
    "#### This function is used to train our agent. The training parameters are as follows:\n",
    "1. Number of tasks 20,000 with each task being sampled from the bandit environment. After each task the environment is reset.\n",
    "2. The trials for each task are limited to 100\n",
    "3. Discount rate is fixed at 0.0\n",
    "4. Learning rate is fixed at 0.01\n",
    "5. Number of bandits are 5\n",
    "\n",
    "Optimizer chosen is an ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e55dfa11-ee17-4eff-be91-eb5bcf4c1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRL2(num_episodes, num_tasks, gamma, env, lr):\n",
    "\n",
    "    MetaLearner = RNNAgent(hiddenSize = 48, outputSize = env.num_actions, layers = 2)\n",
    "    optimizer = optim.Adam(MetaLearner.parameters(), lr)\n",
    "    total_rewards = []\n",
    "    \n",
    "    for i in tqdm(range(num_tasks)):\n",
    "        MetaLearner.hiddenLayerReset()\n",
    "        action_probs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        action = 0\n",
    "        reward = 0\n",
    "        env.reset_env()\n",
    "        for t in range(num_episodes):\n",
    "\n",
    "                phi = createphi(0, action, reward, t)\n",
    "                action_prob = MetaLearner.forward(phi)\n",
    "                action_probs.append(action_prob)\n",
    "                \n",
    "                action_distribution = Categorical(action_prob)\n",
    "                action = action_distribution.sample()\n",
    "                actions.append(action)\n",
    "        \n",
    "                _, reward, _, _ = env.step(action)\n",
    "                rewards.append(reward)\n",
    "            \n",
    "        slow_RL(optimizer, action_probs, rewards, gamma, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12ae04a2-75dd-4fec-b18f-5dfdb9dc9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:22<00:00, 12.15it/s]\n"
     ]
    }
   ],
   "source": [
    "num_tasks = 1000\n",
    "num_episodes = 100\n",
    "gamma = 0.9\n",
    "lr = 0.01\n",
    "num_bandits = 5\n",
    "mean = np.random.normal(size=(1,num_bandits))\n",
    "stddev = np.ones((1,num_bandits))\n",
    "#env = ArmedBanditsEnv(mean, stddev)\n",
    "env = IndependentArms(num_bandits)\n",
    "trainRL2(num_episodes, num_tasks, gamma, env, lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d66423b",
   "metadata": {},
   "source": [
    "#### To test our agent we are going to use the following helper function the test is performed on thousand instances of the bandit problem each lasting for 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c4c0ea04-2fae-42f4-a027-9f93d1402cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRL2(num_episodes, num_tasks, gamma, env, lr):\n",
    "\n",
    "    MetaLearner = RNNAgent(hiddenSize = 48, outputSize = env.num_actions, layers = 2)\n",
    "    optimizer = optim.Adam(MetaLearner.parameters(), lr)\n",
    "    total_rewards = []\n",
    "    #optimal = np.argmax(env.mean, axis = 1)\n",
    "\n",
    "    for i in range(num_tasks):\n",
    "        MetaLearner.hiddenLayerReset()\n",
    "        action_probs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        action = 0\n",
    "        reward = 0\n",
    "        env.reset()\n",
    "        sub_optimal_pulls = 0\n",
    "        \n",
    "        for t in range(num_episodes):\n",
    "\n",
    "                phi = createphi(0, action, reward, t)\n",
    "                action_prob = MetaLearner.forward(phi)\n",
    "                action_probs.append(action_prob)\n",
    "                \n",
    "                action_distribution = Categorical(action_prob)\n",
    "                action = action_distribution.sample()\n",
    "                actions.append(action)\n",
    "                #if action.item() != optimal : sub_optimal_pulls+=1\n",
    "                _, reward, _, _ = env.step(action)\n",
    "                rewards.append(reward)\n",
    "        total_rewards.append(np.sum(rewards))    \n",
    "        slow_RL(optimizer, action_probs, rewards, gamma, action)\n",
    "        printresults(rewards, i, num_tasks, total_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "25015855-fb1e-4bd2-b23d-318dfd2884bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printresults(rewards, i, num_tasks, total_rewards):\n",
    "    print(\"For the task {}/{} Avearge Reward {:.2f}\" .format(i+1, num_tasks, np.mean(total_rewards)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "657da623-ece0-4389-a087-6272be7b2e34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the task 1/1000 Avearge Reward 38.00\n",
      "For the task 2/1000 Avearge Reward 37.00\n",
      "For the task 3/1000 Avearge Reward 35.33\n",
      "For the task 4/1000 Avearge Reward 34.00\n",
      "For the task 5/1000 Avearge Reward 35.20\n",
      "For the task 6/1000 Avearge Reward 36.83\n",
      "For the task 7/1000 Avearge Reward 37.14\n",
      "For the task 8/1000 Avearge Reward 37.50\n",
      "For the task 9/1000 Avearge Reward 37.44\n",
      "For the task 10/1000 Avearge Reward 38.20\n",
      "For the task 11/1000 Avearge Reward 38.73\n",
      "For the task 12/1000 Avearge Reward 38.33\n",
      "For the task 13/1000 Avearge Reward 38.92\n",
      "For the task 14/1000 Avearge Reward 39.07\n",
      "For the task 15/1000 Avearge Reward 39.27\n",
      "For the task 16/1000 Avearge Reward 39.50\n",
      "For the task 17/1000 Avearge Reward 39.12\n",
      "For the task 18/1000 Avearge Reward 39.00\n",
      "For the task 19/1000 Avearge Reward 39.74\n",
      "For the task 20/1000 Avearge Reward 40.15\n",
      "For the task 21/1000 Avearge Reward 40.05\n",
      "For the task 22/1000 Avearge Reward 39.82\n",
      "For the task 23/1000 Avearge Reward 39.83\n",
      "For the task 24/1000 Avearge Reward 39.88\n",
      "For the task 25/1000 Avearge Reward 39.76\n",
      "For the task 26/1000 Avearge Reward 40.23\n",
      "For the task 27/1000 Avearge Reward 40.37\n",
      "For the task 28/1000 Avearge Reward 40.79\n",
      "For the task 29/1000 Avearge Reward 40.38\n",
      "For the task 30/1000 Avearge Reward 40.33\n",
      "For the task 31/1000 Avearge Reward 40.35\n",
      "For the task 32/1000 Avearge Reward 40.38\n",
      "For the task 33/1000 Avearge Reward 40.42\n",
      "For the task 34/1000 Avearge Reward 40.18\n",
      "For the task 35/1000 Avearge Reward 40.34\n",
      "For the task 36/1000 Avearge Reward 40.42\n",
      "For the task 37/1000 Avearge Reward 40.30\n",
      "For the task 38/1000 Avearge Reward 40.50\n",
      "For the task 39/1000 Avearge Reward 40.38\n",
      "For the task 40/1000 Avearge Reward 40.40\n",
      "For the task 41/1000 Avearge Reward 40.51\n",
      "For the task 42/1000 Avearge Reward 40.55\n",
      "For the task 43/1000 Avearge Reward 40.53\n",
      "For the task 44/1000 Avearge Reward 40.55\n",
      "For the task 45/1000 Avearge Reward 40.58\n",
      "For the task 46/1000 Avearge Reward 40.52\n",
      "For the task 47/1000 Avearge Reward 40.62\n",
      "For the task 48/1000 Avearge Reward 40.52\n",
      "For the task 49/1000 Avearge Reward 40.45\n",
      "For the task 50/1000 Avearge Reward 40.38\n",
      "For the task 51/1000 Avearge Reward 40.41\n",
      "For the task 52/1000 Avearge Reward 40.29\n",
      "For the task 53/1000 Avearge Reward 40.25\n",
      "For the task 54/1000 Avearge Reward 40.24\n",
      "For the task 55/1000 Avearge Reward 40.22\n",
      "For the task 56/1000 Avearge Reward 40.32\n",
      "For the task 57/1000 Avearge Reward 40.33\n",
      "For the task 58/1000 Avearge Reward 40.26\n",
      "For the task 59/1000 Avearge Reward 40.22\n",
      "For the task 60/1000 Avearge Reward 40.33\n",
      "For the task 61/1000 Avearge Reward 40.39\n",
      "For the task 62/1000 Avearge Reward 40.44\n",
      "For the task 63/1000 Avearge Reward 40.59\n",
      "For the task 64/1000 Avearge Reward 40.72\n",
      "For the task 65/1000 Avearge Reward 40.74\n",
      "For the task 66/1000 Avearge Reward 40.71\n",
      "For the task 67/1000 Avearge Reward 40.58\n",
      "For the task 68/1000 Avearge Reward 40.59\n",
      "For the task 69/1000 Avearge Reward 40.64\n",
      "For the task 70/1000 Avearge Reward 40.61\n",
      "For the task 71/1000 Avearge Reward 40.62\n",
      "For the task 72/1000 Avearge Reward 40.58\n",
      "For the task 73/1000 Avearge Reward 40.59\n",
      "For the task 74/1000 Avearge Reward 40.62\n",
      "For the task 75/1000 Avearge Reward 40.64\n",
      "For the task 76/1000 Avearge Reward 40.63\n",
      "For the task 77/1000 Avearge Reward 40.61\n",
      "For the task 78/1000 Avearge Reward 40.65\n",
      "For the task 79/1000 Avearge Reward 40.65\n",
      "For the task 80/1000 Avearge Reward 40.62\n",
      "For the task 81/1000 Avearge Reward 40.75\n",
      "For the task 82/1000 Avearge Reward 40.77\n",
      "For the task 83/1000 Avearge Reward 40.78\n",
      "For the task 84/1000 Avearge Reward 40.81\n",
      "For the task 85/1000 Avearge Reward 40.88\n",
      "For the task 86/1000 Avearge Reward 40.83\n",
      "For the task 87/1000 Avearge Reward 40.98\n",
      "For the task 88/1000 Avearge Reward 41.11\n",
      "For the task 89/1000 Avearge Reward 41.09\n",
      "For the task 90/1000 Avearge Reward 41.18\n",
      "For the task 91/1000 Avearge Reward 41.10\n",
      "For the task 92/1000 Avearge Reward 41.12\n",
      "For the task 93/1000 Avearge Reward 41.03\n",
      "For the task 94/1000 Avearge Reward 41.01\n",
      "For the task 95/1000 Avearge Reward 40.97\n",
      "For the task 96/1000 Avearge Reward 40.89\n",
      "For the task 97/1000 Avearge Reward 40.97\n",
      "For the task 98/1000 Avearge Reward 41.00\n",
      "For the task 99/1000 Avearge Reward 41.01\n",
      "For the task 100/1000 Avearge Reward 41.07\n",
      "For the task 101/1000 Avearge Reward 40.92\n",
      "For the task 102/1000 Avearge Reward 40.99\n",
      "For the task 103/1000 Avearge Reward 41.00\n",
      "For the task 104/1000 Avearge Reward 41.02\n",
      "For the task 105/1000 Avearge Reward 41.05\n",
      "For the task 106/1000 Avearge Reward 40.98\n",
      "For the task 107/1000 Avearge Reward 41.06\n",
      "For the task 108/1000 Avearge Reward 41.04\n",
      "For the task 109/1000 Avearge Reward 41.12\n",
      "For the task 110/1000 Avearge Reward 41.07\n",
      "For the task 111/1000 Avearge Reward 41.09\n",
      "For the task 112/1000 Avearge Reward 41.16\n",
      "For the task 113/1000 Avearge Reward 41.10\n",
      "For the task 114/1000 Avearge Reward 41.03\n",
      "For the task 115/1000 Avearge Reward 41.08\n",
      "For the task 116/1000 Avearge Reward 41.03\n",
      "For the task 117/1000 Avearge Reward 41.10\n",
      "For the task 118/1000 Avearge Reward 41.10\n",
      "For the task 119/1000 Avearge Reward 41.09\n",
      "For the task 120/1000 Avearge Reward 41.03\n",
      "For the task 121/1000 Avearge Reward 41.11\n",
      "For the task 122/1000 Avearge Reward 41.11\n",
      "For the task 123/1000 Avearge Reward 41.08\n",
      "For the task 124/1000 Avearge Reward 41.13\n",
      "For the task 125/1000 Avearge Reward 41.10\n",
      "For the task 126/1000 Avearge Reward 41.10\n",
      "For the task 127/1000 Avearge Reward 41.03\n",
      "For the task 128/1000 Avearge Reward 41.05\n",
      "For the task 129/1000 Avearge Reward 41.08\n",
      "For the task 130/1000 Avearge Reward 41.05\n",
      "For the task 131/1000 Avearge Reward 41.05\n",
      "For the task 132/1000 Avearge Reward 41.08\n",
      "For the task 133/1000 Avearge Reward 41.09\n",
      "For the task 134/1000 Avearge Reward 41.04\n",
      "For the task 135/1000 Avearge Reward 40.99\n",
      "For the task 136/1000 Avearge Reward 40.97\n",
      "For the task 137/1000 Avearge Reward 40.97\n",
      "For the task 138/1000 Avearge Reward 41.03\n",
      "For the task 139/1000 Avearge Reward 41.04\n",
      "For the task 140/1000 Avearge Reward 41.01\n",
      "For the task 141/1000 Avearge Reward 41.02\n",
      "For the task 142/1000 Avearge Reward 41.06\n",
      "For the task 143/1000 Avearge Reward 41.05\n",
      "For the task 144/1000 Avearge Reward 41.03\n",
      "For the task 145/1000 Avearge Reward 41.02\n",
      "For the task 146/1000 Avearge Reward 41.01\n",
      "For the task 147/1000 Avearge Reward 41.01\n",
      "For the task 148/1000 Avearge Reward 41.00\n",
      "For the task 149/1000 Avearge Reward 40.96\n",
      "For the task 150/1000 Avearge Reward 40.95\n",
      "For the task 151/1000 Avearge Reward 40.93\n",
      "For the task 152/1000 Avearge Reward 40.93\n",
      "For the task 153/1000 Avearge Reward 40.92\n",
      "For the task 154/1000 Avearge Reward 40.92\n",
      "For the task 155/1000 Avearge Reward 40.97\n",
      "For the task 156/1000 Avearge Reward 40.99\n",
      "For the task 157/1000 Avearge Reward 40.98\n",
      "For the task 158/1000 Avearge Reward 40.95\n",
      "For the task 159/1000 Avearge Reward 40.92\n",
      "For the task 160/1000 Avearge Reward 40.90\n",
      "For the task 161/1000 Avearge Reward 40.88\n",
      "For the task 162/1000 Avearge Reward 40.92\n",
      "For the task 163/1000 Avearge Reward 40.84\n",
      "For the task 164/1000 Avearge Reward 40.82\n",
      "For the task 165/1000 Avearge Reward 40.80\n",
      "For the task 166/1000 Avearge Reward 40.81\n",
      "For the task 167/1000 Avearge Reward 40.78\n",
      "For the task 168/1000 Avearge Reward 40.76\n",
      "For the task 169/1000 Avearge Reward 40.83\n",
      "For the task 170/1000 Avearge Reward 40.86\n",
      "For the task 171/1000 Avearge Reward 40.83\n",
      "For the task 172/1000 Avearge Reward 40.87\n",
      "For the task 173/1000 Avearge Reward 40.85\n",
      "For the task 174/1000 Avearge Reward 40.82\n",
      "For the task 175/1000 Avearge Reward 40.82\n",
      "For the task 176/1000 Avearge Reward 40.85\n",
      "For the task 177/1000 Avearge Reward 40.85\n",
      "For the task 178/1000 Avearge Reward 40.82\n",
      "For the task 179/1000 Avearge Reward 40.79\n",
      "For the task 180/1000 Avearge Reward 40.81\n",
      "For the task 181/1000 Avearge Reward 40.83\n",
      "For the task 182/1000 Avearge Reward 40.79\n",
      "For the task 183/1000 Avearge Reward 40.80\n",
      "For the task 184/1000 Avearge Reward 40.80\n",
      "For the task 185/1000 Avearge Reward 40.82\n",
      "For the task 186/1000 Avearge Reward 40.82\n",
      "For the task 187/1000 Avearge Reward 40.83\n",
      "For the task 188/1000 Avearge Reward 40.78\n",
      "For the task 189/1000 Avearge Reward 40.77\n",
      "For the task 190/1000 Avearge Reward 40.83\n",
      "For the task 191/1000 Avearge Reward 40.82\n",
      "For the task 192/1000 Avearge Reward 40.81\n",
      "For the task 193/1000 Avearge Reward 40.80\n",
      "For the task 194/1000 Avearge Reward 40.77\n",
      "For the task 195/1000 Avearge Reward 40.80\n",
      "For the task 196/1000 Avearge Reward 40.79\n",
      "For the task 197/1000 Avearge Reward 40.79\n",
      "For the task 198/1000 Avearge Reward 40.76\n",
      "For the task 199/1000 Avearge Reward 40.76\n",
      "For the task 200/1000 Avearge Reward 40.76\n",
      "For the task 201/1000 Avearge Reward 40.80\n",
      "For the task 202/1000 Avearge Reward 40.78\n",
      "For the task 203/1000 Avearge Reward 40.75\n",
      "For the task 204/1000 Avearge Reward 40.73\n",
      "For the task 205/1000 Avearge Reward 40.75\n",
      "For the task 206/1000 Avearge Reward 40.77\n",
      "For the task 207/1000 Avearge Reward 40.77\n",
      "For the task 208/1000 Avearge Reward 40.79\n",
      "For the task 209/1000 Avearge Reward 40.78\n",
      "For the task 210/1000 Avearge Reward 40.81\n",
      "For the task 211/1000 Avearge Reward 40.80\n",
      "For the task 212/1000 Avearge Reward 40.76\n",
      "For the task 213/1000 Avearge Reward 40.77\n",
      "For the task 214/1000 Avearge Reward 40.77\n",
      "For the task 215/1000 Avearge Reward 40.74\n",
      "For the task 216/1000 Avearge Reward 40.76\n",
      "For the task 217/1000 Avearge Reward 40.77\n",
      "For the task 218/1000 Avearge Reward 40.80\n",
      "For the task 219/1000 Avearge Reward 40.81\n",
      "For the task 220/1000 Avearge Reward 40.79\n",
      "For the task 221/1000 Avearge Reward 40.81\n",
      "For the task 222/1000 Avearge Reward 40.82\n",
      "For the task 223/1000 Avearge Reward 40.80\n",
      "For the task 224/1000 Avearge Reward 40.83\n",
      "For the task 225/1000 Avearge Reward 40.81\n",
      "For the task 226/1000 Avearge Reward 40.81\n",
      "For the task 227/1000 Avearge Reward 40.85\n",
      "For the task 228/1000 Avearge Reward 40.82\n",
      "For the task 229/1000 Avearge Reward 40.81\n",
      "For the task 230/1000 Avearge Reward 40.82\n",
      "For the task 231/1000 Avearge Reward 40.78\n",
      "For the task 232/1000 Avearge Reward 40.79\n",
      "For the task 233/1000 Avearge Reward 40.79\n",
      "For the task 234/1000 Avearge Reward 40.83\n",
      "For the task 235/1000 Avearge Reward 40.83\n",
      "For the task 236/1000 Avearge Reward 40.83\n",
      "For the task 237/1000 Avearge Reward 40.82\n",
      "For the task 238/1000 Avearge Reward 40.83\n",
      "For the task 239/1000 Avearge Reward 40.85\n",
      "For the task 240/1000 Avearge Reward 40.85\n",
      "For the task 241/1000 Avearge Reward 40.82\n",
      "For the task 242/1000 Avearge Reward 40.81\n",
      "For the task 243/1000 Avearge Reward 40.83\n",
      "For the task 244/1000 Avearge Reward 40.84\n",
      "For the task 245/1000 Avearge Reward 40.86\n",
      "For the task 246/1000 Avearge Reward 40.87\n",
      "For the task 247/1000 Avearge Reward 40.91\n",
      "For the task 248/1000 Avearge Reward 40.92\n",
      "For the task 249/1000 Avearge Reward 40.90\n",
      "For the task 250/1000 Avearge Reward 40.92\n",
      "For the task 251/1000 Avearge Reward 40.90\n",
      "For the task 252/1000 Avearge Reward 40.89\n",
      "For the task 253/1000 Avearge Reward 40.93\n",
      "For the task 254/1000 Avearge Reward 40.92\n",
      "For the task 255/1000 Avearge Reward 40.91\n",
      "For the task 256/1000 Avearge Reward 40.92\n",
      "For the task 257/1000 Avearge Reward 40.93\n",
      "For the task 258/1000 Avearge Reward 40.92\n",
      "For the task 259/1000 Avearge Reward 40.92\n",
      "For the task 260/1000 Avearge Reward 40.92\n",
      "For the task 261/1000 Avearge Reward 40.92\n",
      "For the task 262/1000 Avearge Reward 40.94\n",
      "For the task 263/1000 Avearge Reward 40.92\n",
      "For the task 264/1000 Avearge Reward 40.90\n",
      "For the task 265/1000 Avearge Reward 40.87\n",
      "For the task 266/1000 Avearge Reward 40.84\n",
      "For the task 267/1000 Avearge Reward 40.86\n",
      "For the task 268/1000 Avearge Reward 40.86\n",
      "For the task 269/1000 Avearge Reward 40.87\n",
      "For the task 270/1000 Avearge Reward 40.84\n",
      "For the task 271/1000 Avearge Reward 40.84\n",
      "For the task 272/1000 Avearge Reward 40.86\n",
      "For the task 273/1000 Avearge Reward 40.89\n",
      "For the task 274/1000 Avearge Reward 40.87\n",
      "For the task 275/1000 Avearge Reward 40.89\n",
      "For the task 276/1000 Avearge Reward 40.89\n",
      "For the task 277/1000 Avearge Reward 40.87\n",
      "For the task 278/1000 Avearge Reward 40.84\n",
      "For the task 279/1000 Avearge Reward 40.82\n",
      "For the task 280/1000 Avearge Reward 40.85\n",
      "For the task 281/1000 Avearge Reward 40.86\n",
      "For the task 282/1000 Avearge Reward 40.87\n",
      "For the task 283/1000 Avearge Reward 40.89\n",
      "For the task 284/1000 Avearge Reward 40.90\n",
      "For the task 285/1000 Avearge Reward 40.94\n",
      "For the task 286/1000 Avearge Reward 40.95\n",
      "For the task 287/1000 Avearge Reward 40.97\n",
      "For the task 288/1000 Avearge Reward 40.96\n",
      "For the task 289/1000 Avearge Reward 40.96\n",
      "For the task 290/1000 Avearge Reward 40.94\n",
      "For the task 291/1000 Avearge Reward 40.95\n",
      "For the task 292/1000 Avearge Reward 40.97\n",
      "For the task 293/1000 Avearge Reward 40.97\n",
      "For the task 294/1000 Avearge Reward 40.99\n",
      "For the task 295/1000 Avearge Reward 40.99\n",
      "For the task 296/1000 Avearge Reward 40.98\n",
      "For the task 297/1000 Avearge Reward 40.98\n",
      "For the task 298/1000 Avearge Reward 40.98\n",
      "For the task 299/1000 Avearge Reward 40.97\n",
      "For the task 300/1000 Avearge Reward 40.98\n",
      "For the task 301/1000 Avearge Reward 40.99\n",
      "For the task 302/1000 Avearge Reward 40.97\n",
      "For the task 303/1000 Avearge Reward 40.97\n",
      "For the task 304/1000 Avearge Reward 40.95\n",
      "For the task 305/1000 Avearge Reward 40.97\n",
      "For the task 306/1000 Avearge Reward 40.94\n",
      "For the task 307/1000 Avearge Reward 40.94\n",
      "For the task 308/1000 Avearge Reward 40.95\n",
      "For the task 309/1000 Avearge Reward 40.97\n",
      "For the task 310/1000 Avearge Reward 40.98\n",
      "For the task 311/1000 Avearge Reward 40.96\n",
      "For the task 312/1000 Avearge Reward 40.94\n",
      "For the task 313/1000 Avearge Reward 40.90\n",
      "For the task 314/1000 Avearge Reward 40.87\n",
      "For the task 315/1000 Avearge Reward 40.88\n",
      "For the task 316/1000 Avearge Reward 40.85\n",
      "For the task 317/1000 Avearge Reward 40.82\n",
      "For the task 318/1000 Avearge Reward 40.82\n",
      "For the task 319/1000 Avearge Reward 40.81\n",
      "For the task 320/1000 Avearge Reward 40.78\n",
      "For the task 321/1000 Avearge Reward 40.76\n",
      "For the task 322/1000 Avearge Reward 40.79\n",
      "For the task 323/1000 Avearge Reward 40.80\n",
      "For the task 324/1000 Avearge Reward 40.81\n",
      "For the task 325/1000 Avearge Reward 40.80\n",
      "For the task 326/1000 Avearge Reward 40.78\n",
      "For the task 327/1000 Avearge Reward 40.78\n",
      "For the task 328/1000 Avearge Reward 40.78\n",
      "For the task 329/1000 Avearge Reward 40.80\n",
      "For the task 330/1000 Avearge Reward 40.78\n",
      "For the task 331/1000 Avearge Reward 40.79\n",
      "For the task 332/1000 Avearge Reward 40.80\n",
      "For the task 333/1000 Avearge Reward 40.80\n",
      "For the task 334/1000 Avearge Reward 40.82\n",
      "For the task 335/1000 Avearge Reward 40.81\n",
      "For the task 336/1000 Avearge Reward 40.81\n",
      "For the task 337/1000 Avearge Reward 40.81\n",
      "For the task 338/1000 Avearge Reward 40.80\n",
      "For the task 339/1000 Avearge Reward 40.81\n",
      "For the task 340/1000 Avearge Reward 40.80\n",
      "For the task 341/1000 Avearge Reward 40.79\n",
      "For the task 342/1000 Avearge Reward 40.78\n",
      "For the task 343/1000 Avearge Reward 40.78\n",
      "For the task 344/1000 Avearge Reward 40.79\n",
      "For the task 345/1000 Avearge Reward 40.79\n",
      "For the task 346/1000 Avearge Reward 40.80\n",
      "For the task 347/1000 Avearge Reward 40.82\n",
      "For the task 348/1000 Avearge Reward 40.80\n",
      "For the task 349/1000 Avearge Reward 40.81\n",
      "For the task 350/1000 Avearge Reward 40.81\n",
      "For the task 351/1000 Avearge Reward 40.83\n",
      "For the task 352/1000 Avearge Reward 40.81\n",
      "For the task 353/1000 Avearge Reward 40.82\n",
      "For the task 354/1000 Avearge Reward 40.81\n",
      "For the task 355/1000 Avearge Reward 40.84\n",
      "For the task 356/1000 Avearge Reward 40.83\n",
      "For the task 357/1000 Avearge Reward 40.82\n",
      "For the task 358/1000 Avearge Reward 40.84\n",
      "For the task 359/1000 Avearge Reward 40.82\n",
      "For the task 360/1000 Avearge Reward 40.81\n",
      "For the task 361/1000 Avearge Reward 40.84\n",
      "For the task 362/1000 Avearge Reward 40.81\n",
      "For the task 363/1000 Avearge Reward 40.82\n",
      "For the task 364/1000 Avearge Reward 40.80\n",
      "For the task 365/1000 Avearge Reward 40.81\n",
      "For the task 366/1000 Avearge Reward 40.80\n",
      "For the task 367/1000 Avearge Reward 40.79\n",
      "For the task 368/1000 Avearge Reward 40.80\n",
      "For the task 369/1000 Avearge Reward 40.82\n",
      "For the task 370/1000 Avearge Reward 40.84\n",
      "For the task 371/1000 Avearge Reward 40.82\n",
      "For the task 372/1000 Avearge Reward 40.83\n",
      "For the task 373/1000 Avearge Reward 40.82\n",
      "For the task 374/1000 Avearge Reward 40.81\n",
      "For the task 375/1000 Avearge Reward 40.78\n",
      "For the task 376/1000 Avearge Reward 40.75\n",
      "For the task 377/1000 Avearge Reward 40.76\n",
      "For the task 378/1000 Avearge Reward 40.75\n",
      "For the task 379/1000 Avearge Reward 40.74\n",
      "For the task 380/1000 Avearge Reward 40.72\n",
      "For the task 381/1000 Avearge Reward 40.70\n",
      "For the task 382/1000 Avearge Reward 40.71\n",
      "For the task 383/1000 Avearge Reward 40.71\n",
      "For the task 384/1000 Avearge Reward 40.71\n",
      "For the task 385/1000 Avearge Reward 40.70\n",
      "For the task 386/1000 Avearge Reward 40.70\n",
      "For the task 387/1000 Avearge Reward 40.71\n",
      "For the task 388/1000 Avearge Reward 40.71\n",
      "For the task 389/1000 Avearge Reward 40.71\n",
      "For the task 390/1000 Avearge Reward 40.73\n",
      "For the task 391/1000 Avearge Reward 40.73\n",
      "For the task 392/1000 Avearge Reward 40.72\n",
      "For the task 393/1000 Avearge Reward 40.73\n",
      "For the task 394/1000 Avearge Reward 40.73\n",
      "For the task 395/1000 Avearge Reward 40.74\n",
      "For the task 396/1000 Avearge Reward 40.76\n",
      "For the task 397/1000 Avearge Reward 40.77\n",
      "For the task 398/1000 Avearge Reward 40.76\n",
      "For the task 399/1000 Avearge Reward 40.76\n",
      "For the task 400/1000 Avearge Reward 40.77\n",
      "For the task 401/1000 Avearge Reward 40.76\n",
      "For the task 402/1000 Avearge Reward 40.78\n",
      "For the task 403/1000 Avearge Reward 40.79\n",
      "For the task 404/1000 Avearge Reward 40.80\n",
      "For the task 405/1000 Avearge Reward 40.79\n",
      "For the task 406/1000 Avearge Reward 40.81\n",
      "For the task 407/1000 Avearge Reward 40.82\n",
      "For the task 408/1000 Avearge Reward 40.83\n",
      "For the task 409/1000 Avearge Reward 40.83\n",
      "For the task 410/1000 Avearge Reward 40.86\n",
      "For the task 411/1000 Avearge Reward 40.87\n",
      "For the task 412/1000 Avearge Reward 40.87\n",
      "For the task 413/1000 Avearge Reward 40.87\n",
      "For the task 414/1000 Avearge Reward 40.88\n",
      "For the task 415/1000 Avearge Reward 40.87\n",
      "For the task 416/1000 Avearge Reward 40.89\n",
      "For the task 417/1000 Avearge Reward 40.88\n",
      "For the task 418/1000 Avearge Reward 40.88\n",
      "For the task 419/1000 Avearge Reward 40.84\n",
      "For the task 420/1000 Avearge Reward 40.85\n",
      "For the task 421/1000 Avearge Reward 40.85\n",
      "For the task 422/1000 Avearge Reward 40.86\n",
      "For the task 423/1000 Avearge Reward 40.88\n",
      "For the task 424/1000 Avearge Reward 40.86\n",
      "For the task 425/1000 Avearge Reward 40.88\n",
      "For the task 426/1000 Avearge Reward 40.88\n",
      "For the task 427/1000 Avearge Reward 40.86\n",
      "For the task 428/1000 Avearge Reward 40.86\n",
      "For the task 429/1000 Avearge Reward 40.86\n",
      "For the task 430/1000 Avearge Reward 40.86\n",
      "For the task 431/1000 Avearge Reward 40.88\n",
      "For the task 432/1000 Avearge Reward 40.85\n",
      "For the task 433/1000 Avearge Reward 40.84\n",
      "For the task 434/1000 Avearge Reward 40.84\n",
      "For the task 435/1000 Avearge Reward 40.84\n",
      "For the task 436/1000 Avearge Reward 40.84\n",
      "For the task 437/1000 Avearge Reward 40.81\n",
      "For the task 438/1000 Avearge Reward 40.81\n",
      "For the task 439/1000 Avearge Reward 40.82\n",
      "For the task 440/1000 Avearge Reward 40.83\n",
      "For the task 441/1000 Avearge Reward 40.82\n",
      "For the task 442/1000 Avearge Reward 40.82\n",
      "For the task 443/1000 Avearge Reward 40.82\n",
      "For the task 444/1000 Avearge Reward 40.82\n",
      "For the task 445/1000 Avearge Reward 40.82\n",
      "For the task 446/1000 Avearge Reward 40.82\n",
      "For the task 447/1000 Avearge Reward 40.83\n",
      "For the task 448/1000 Avearge Reward 40.83\n",
      "For the task 449/1000 Avearge Reward 40.82\n",
      "For the task 450/1000 Avearge Reward 40.80\n",
      "For the task 451/1000 Avearge Reward 40.83\n",
      "For the task 452/1000 Avearge Reward 40.84\n",
      "For the task 453/1000 Avearge Reward 40.83\n",
      "For the task 454/1000 Avearge Reward 40.84\n",
      "For the task 455/1000 Avearge Reward 40.83\n",
      "For the task 456/1000 Avearge Reward 40.81\n",
      "For the task 457/1000 Avearge Reward 40.82\n",
      "For the task 458/1000 Avearge Reward 40.81\n",
      "For the task 459/1000 Avearge Reward 40.80\n",
      "For the task 460/1000 Avearge Reward 40.80\n",
      "For the task 461/1000 Avearge Reward 40.81\n",
      "For the task 462/1000 Avearge Reward 40.81\n",
      "For the task 463/1000 Avearge Reward 40.79\n",
      "For the task 464/1000 Avearge Reward 40.80\n",
      "For the task 465/1000 Avearge Reward 40.80\n",
      "For the task 466/1000 Avearge Reward 40.81\n",
      "For the task 467/1000 Avearge Reward 40.81\n",
      "For the task 468/1000 Avearge Reward 40.81\n",
      "For the task 469/1000 Avearge Reward 40.79\n",
      "For the task 470/1000 Avearge Reward 40.79\n",
      "For the task 471/1000 Avearge Reward 40.79\n",
      "For the task 472/1000 Avearge Reward 40.79\n",
      "For the task 473/1000 Avearge Reward 40.80\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[39m#env = ArmedBanditsEnv(mean, stddev)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m env \u001b[39m=\u001b[39m IndependentArms(num_bandits)\n\u001b[1;32m----> 6\u001b[0m testRL2(\u001b[39m100\u001b[39;49m, \u001b[39m1000\u001b[39;49m, gamma, env, lr\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[38], line 24\u001b[0m, in \u001b[0;36mtestRL2\u001b[1;34m(num_episodes, num_tasks, gamma, env, lr)\u001b[0m\n\u001b[0;32m     21\u001b[0m action_prob \u001b[39m=\u001b[39m MetaLearner\u001b[39m.\u001b[39mforward(phi)\n\u001b[0;32m     22\u001b[0m action_probs\u001b[39m.\u001b[39mappend(action_prob)\n\u001b[1;32m---> 24\u001b[0m action_distribution \u001b[39m=\u001b[39m Categorical(action_prob)\n\u001b[0;32m     25\u001b[0m action \u001b[39m=\u001b[39m action_distribution\u001b[39m.\u001b[39msample()\n\u001b[0;32m     26\u001b[0m actions\u001b[39m.\u001b[39mappend(action)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\distributions\\categorical.py:66\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[1;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     65\u001b[0m batch_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39mndimension() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mSize()\n\u001b[1;32m---> 66\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\distributions\\distribution.py:60\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# skip checking lazily-constructed args\u001b[39;00m\n\u001b[0;32m     59\u001b[0m value \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, param)\n\u001b[1;32m---> 60\u001b[0m valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39;49mcheck(value)\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m     62\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     63\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\torch\\distributions\\constraints.py:406\u001b[0m, in \u001b[0;36m_Simplex.check\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    405\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcheck\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m--> 406\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mall(value \u001b[39m>\u001b[39;49m\u001b[39m=\u001b[39;49m \u001b[39m0\u001b[39;49m, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m&\u001b[39m ((value\u001b[39m.\u001b[39msum(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mabs() \u001b[39m<\u001b[39m \u001b[39m1e-6\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_bandits = 5\n",
    "mean = np.random.normal(size=(1,num_bandits))\n",
    "stddev = np.ones((1,num_bandits))\n",
    "#env = ArmedBanditsEnv(mean, stddev)\n",
    "env = IndependentArms(num_bandits)\n",
    "testRL2(100, 1000, gamma, env, lr=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d60567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB():\n",
    "    def __init__(self, env):\n",
    "        self.c = 2\n",
    "        self.num_actions = env.num_actions\n",
    "        self.ucb = np.zeros(self.num_actions)\n",
    "        self.action_count = np.zeros(self.num_actions)\n",
    "        self.action_step = 1\n",
    "        self.reward_estimates = np.zeros(self.num_actions)\n",
    "        self.env = env\n",
    "    \n",
    "    def name():\n",
    "        return 'ucb'\n",
    "\n",
    "    def get_action(self):\n",
    "        action = np.argmax(self.reward_estimates + self.c * np.sqrt(np.log(np.full(self.num_actions, self.action_step))/self.action_count))\n",
    "        self.action_step += 1\n",
    "        return action\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        _, reward, _, _ = self.env.step(action)\n",
    "        return reward\n",
    "        \n",
    "    def update_estimates(self, action, reward):\n",
    "        self.action_count[action] += 1\n",
    "        self.reward_estimates[action] += 1/self.action_count[action] *(reward - self.reward_estimates[action])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d40ba7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling():\n",
    "    def __init__(self, env):\n",
    "        self.num_actions = env.num_actions\n",
    "        self.alpha = np.ones(self.num_actions)\n",
    "        self.beta = np.ones(self.num_actions)\n",
    "        self.env = env\n",
    "    \n",
    "    def get_reward(self, action):\n",
    "        _, reward, _, _ = self.env.step(action)\n",
    "        self.alpha[action] += reward\n",
    "        self.beta[action] += 1 - reward\n",
    "        \n",
    "        return reward\n",
    "    def name():\n",
    "        return 'thompson'\n",
    "\n",
    "    def get_action(self):\n",
    "        action = np.argmax(np.random.beta(self.alpha, self.beta))\n",
    "        \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0673aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to plot data\n",
    "\n",
    "def plot(data):\n",
    "    data_x = np.arange(data.size)\n",
    "    p = plt.plot(x, y, o)\n",
    "\n",
    "def plot_data(data, type):\n",
    "\n",
    "    x = np.arange(data[0].size)\n",
    "    for i, y in enumerate(data):\n",
    "        plt.plot(x, y, 'o', markersize = 2, label = type[i])\n",
    "    plt.legend(loc = 'upper right', prop = {'size': 16}, numpoints = 10)\n",
    "    plt.show()\n",
    "\n",
    "def episodic(episodes, trials, num_actions, algo):\n",
    "\n",
    "    sum_rewards = np.zeros(trials)\n",
    "\n",
    "    for e in range(episodes):\n",
    "        env = IndependentArms(num_actions)\n",
    "        algorithm = algo(env)\n",
    "        rewards = np.zeros(trials)\n",
    "\n",
    "        for i in range(trials):\n",
    "            action = algorithm.get_action()\n",
    "            reward = algorithm.get_reward(action)\n",
    "\n",
    "            rewards[i] = reward\n",
    "        sum_rewards += rewards\n",
    "    average_reward = sum_rewards / episodes\n",
    "    \n",
    "    return average_reward\n",
    "\n",
    "def experiments(num_actions, trials, episodes):\n",
    "\n",
    "    algorithms = [UCB, ThompsonSampling]\n",
    "    rewards_exp = []\n",
    "    names = []\n",
    "\n",
    "    for algo in algorithms:\n",
    "        rewards_exp.append(episodic(episodes, trials, num_actions, algo))\n",
    "        names.append(algo.name())\n",
    "    plot_data(rewards_exp, names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af71228b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Swastik Sharma\\AppData\\Local\\Temp\\ipykernel_14396\\3329132642.py:15: RuntimeWarning: invalid value encountered in divide\n",
      "  action = np.argmax(self.reward_estimates + self.c * np.sqrt(np.log(np.full(self.num_actions, self.action_step))/self.action_count))\n",
      "C:\\Users\\Swastik Sharma\\AppData\\Local\\Temp\\ipykernel_14396\\3329132642.py:15: RuntimeWarning: divide by zero encountered in divide\n",
      "  action = np.argmax(self.reward_estimates + self.c * np.sqrt(np.log(np.full(self.num_actions, self.action_step))/self.action_count))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "num_actions = 5\n",
    "trials = 1000\n",
    "episodes = 100\n",
    "experiments(num_actions, trials, episodes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
