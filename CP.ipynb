{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60a5638",
   "metadata": {},
   "source": [
    "## Implementation of $RL^2$: Fast Reinforcement Learning via Slow Reinforcement Learning\n",
    "The goal of this implementation is to use an RNN agent to be able to learn in itself a Reinforcement Learning algorithm that is able to perfect the exploration and exploitation dilemma of the Multi-Armed Bandit context (Meta-RL). The algorithm is tested in multiple environments and is tested for efficacy with other state of the art agents.\n",
    "\n",
    "1. The implemenation uses a Recurrent Neural Network (RNN) which is a Gated Recurrent Unit (GRU) being fed the last action, reward and timestep.\n",
    "2. The policy is optimized using the basic REINFORCE or Vanilla Policy Gradient.\n",
    "3. Training is being performed for 20,000 tasks(environments) where each task is picked from a random and uniform distribution of means and standard deviations.!\n",
    "\n",
    "Following image shows the visualization of our implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca1043",
   "metadata": {},
   "source": [
    "![Meta RL Implementation](Implementation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ec03d1-a0fe-4691-adae-ba36f0aece17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b095bb",
   "metadata": {},
   "source": [
    "#### The classes below are the environments that are used to test the efficacy of our model and the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f56baef4-7df3-406d-a090-a68a36628136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmedBanditsEnv():\n",
    "    def __init__(self, mean, stddev):\n",
    "        assert len(mean.shape) == 2 \n",
    "        assert len(stddev.shape) == 2\n",
    "        self.num_actions = mean.shape[1] \n",
    "        self.num_experiments = mean.shape[0]\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "        \n",
    "    def step(self, action):\n",
    "    \n",
    "        sampled_means = self.mean[np.arange(self.num_experiments),action]\n",
    "        sampled_stddevs = self.stddev[np.arange(self.num_experiments),action]\n",
    "        \n",
    "        reward = np.random.normal(loc=sampled_means, scale=sampled_stddevs, size=(1,self.num_experiments))\n",
    "        \n",
    "        observation, done, info = 0, False, dict()\n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return 0\n",
    "    \n",
    "    def reset_env(self):\n",
    "        self.mean = np.random.normal(size = (1, self.num_actions))\n",
    "        self.stddev = np.ones((1, self.num_actions))\n",
    "        \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np.random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "229ef643-1706-4145-b20d-f4b2c8cd37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmBanditBernoulli():\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "\n",
    "    def reset_env(self):\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "\n",
    "    def reset(self):\n",
    "        return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = np.random.binomial(n = 1, p = self.probs[action], size=1)[0]\n",
    "        observation, done, info = 0, False, dict()\n",
    "        return observation, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3850d6d-f150-4910-aa4e-55872c45386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependentArms():\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "\n",
    "    def reset_env(self):\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 1 if random.uniform(0,1) <= self.probs[action] else 0\n",
    "        observation, done, info = 0, False, dict()\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ae23c",
   "metadata": {},
   "source": [
    "#### Following classes define our RNN Agent. The agent is a neural network with the input layer a linear layer with an input size of 3 as it takes as input the last action, last reward and timestep a tuple $<a,r,t>$  and the second layer is a GRU with a hidden size of 48, and 2 hidden layers, the output layer is again a linear layer with an ouput size of the number of bandits or actions to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "385c29cb-f450-4132-9b9e-5968d9bc75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAgent(nn.Module):\n",
    "    def __init__(self, hiddenSize, layers,  outputSize, inputSize = 3, timesteps = 1):\n",
    "        super(RNNAgent, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.layers = layers\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "\n",
    "        self.layer1 = nn.Linear(inputSize, hiddenSize)\n",
    "        self.layer2 = nn.GRU(hiddenSize, hiddenSize, layers)\n",
    "        self.layer3 = nn.Linear(hiddenSize, outputSize)\n",
    "\n",
    "        self.hiddenLayerReset()\n",
    "    def name():\n",
    "        return 'rnn'\n",
    "\n",
    "    def hiddenLayerReset(self):\n",
    "        self.hiddenLayer = self.hidden()\n",
    "    \n",
    "    def hidden(self):\n",
    "        layers = self.layers\n",
    "        hiddenSize = self.hiddenSize\n",
    "        timesteps   = self.timesteps\n",
    "\n",
    "        return torch.randn(layers, timesteps, hiddenSize)\n",
    "    \n",
    "    # def get_reward(self, action):\n",
    "    #     _, reward, _, _ = self.env.step(action)\n",
    "\n",
    "    def name():\n",
    "        return 'rnn'\n",
    "\n",
    "    def forward(self, phi):\n",
    "\n",
    "        inputs = torch.relu(self.layer1(phi))\n",
    "        outputs, self.hiddenLayer = self.layer2(inputs, self.hiddenLayer)\n",
    "        logits = torch.relu(self.layer3(outputs))\n",
    "        action_probs = F.softmax(logits, dim = 2).view(-1)\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b794e",
   "metadata": {},
   "source": [
    "#### The input to the RNN is a tuple that of $<s,a,r,t>$ and is represented as $\\phi$ Since for the armed bandit is stateless we'll ignore the state for now and that $\\phi$ is a tuple of the values $<a,r,t>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a2c43f8-527d-4945-a9c0-fa04d454cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createphi(state, action, reward, t):\n",
    "    phi = [action, reward, t]\n",
    "    phi = np.reshape(phi, (1,1,3)).astype(np.float32)\n",
    "    \n",
    "    return torch.tensor(phi)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a4a150c",
   "metadata": {},
   "source": [
    "#### This is our \"slow\" policy optimization algorithm which is a simple REINFORCE policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d02c93c9-826f-4d48-9ea2-1f6d1215eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_RL(optimizer, action_probs, rewards, gamma, action):\n",
    "    #We are using REINFORCE policy gradient here\n",
    "    Gt = 0\n",
    "    discountedRewards = []\n",
    "    policyLoss = []\n",
    "    \n",
    "    for i in reversed(rewards):\n",
    "        Gt = i + gamma * Gt\n",
    "        discountedRewards.insert(0, Gt)\n",
    "    discountedRewards = torch.tensor(discountedRewards)\n",
    "\n",
    "    for action_prob, r in zip(action_probs, discountedRewards):\n",
    "        policyLoss.append(-action_prob * r)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    policyLoss = torch.cat(policyLoss).sum()\n",
    "    policyLoss.backward(retain_graph=True)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217bae4",
   "metadata": {},
   "source": [
    "#### This function is used to train our agent. The training parameters are as follows:\n",
    "1. Number of tasks 20,000 with each task being sampled from the bandit environment. After each task the environment is reset.\n",
    "2. The trials for each task are limited to 100\n",
    "3. Discount rate is fixed at 0.0\n",
    "4. Learning rate is fixed at 0.01\n",
    "5. Number of bandits are 5\n",
    "\n",
    "Optimizer chosen is an ADAM optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e55dfa11-ee17-4eff-be91-eb5bcf4c1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRL2(num_tasks, num_episodes, gamma, env, lr):\n",
    "\n",
    "    MetaLearner = RNNAgent(hiddenSize = 48, outputSize = env.num_actions, layers = 2)\n",
    "    optimizer = optim.Adam(MetaLearner.parameters(), lr)\n",
    "    total_rewards = []\n",
    "    \n",
    "    for i in tqdm(range(num_tasks)):\n",
    "        MetaLearner.hiddenLayerReset()\n",
    "        action_probs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        action = 0\n",
    "        reward = 0\n",
    "        env.reset_env()\n",
    "        for t in range(num_episodes):\n",
    "\n",
    "                phi = createphi(0, action, reward, t)\n",
    "                action_prob = MetaLearner.forward(phi)\n",
    "                action_probs.append(action_prob)\n",
    "                \n",
    "                action_distribution = Categorical(action_prob)\n",
    "                action = action_distribution.sample()\n",
    "                actions.append(action)\n",
    "        \n",
    "                _, reward, _, _ = env.step(action)\n",
    "                rewards.append(reward)\n",
    "            \n",
    "        slow_RL(optimizer, action_probs, rewards, gamma, action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "12ae04a2-75dd-4fec-b18f-5dfdb9dc9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [05:24<00:00, 30.83it/s]\n"
     ]
    }
   ],
   "source": [
    "num_tasks = 10000\n",
    "num_episodes = 100\n",
    "gamma = 0.9\n",
    "lr = 0.01\n",
    "num_bandits = 5\n",
    "mean = np.random.normal(size=(1,num_bandits))\n",
    "stddev = np.ones((1,num_bandits))\n",
    "#env = ArmedBanditsEnv(mean, stddev)\n",
    "env = IndependentArms(num_bandits)\n",
    "trainRL2(num_tasks, num_episodes, gamma, env, lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d66423b",
   "metadata": {},
   "source": [
    "#### To test our agent we are going to use the following helper function, the test is performed on a thousand instances of the bandit problem each lasting for 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c4c0ea04-2fae-42f4-a027-9f93d1402cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRL2(num_tasks, num_episodes, gamma, env, lr):\n",
    "\n",
    "    MetaLearner = RNNAgent(hiddenSize = 48, outputSize = env.num_actions, layers = 2)\n",
    "    optimizer = optim.Adam(MetaLearner.parameters(), lr)\n",
    "    \n",
    "    total_rewards = np.zeros(num_episodes)\n",
    "    \n",
    "    for i in range(num_tasks):\n",
    "        MetaLearner.hiddenLayerReset()\n",
    "        action_probs = []\n",
    "        actions = []\n",
    "        rewards = np.zeros(num_episodes)\n",
    "        action = 0\n",
    "        reward = 0\n",
    "        env.reset()\n",
    "        \n",
    "        for t in range(num_episodes):\n",
    "\n",
    "            phi = createphi(0, action, reward, t)\n",
    "            action_prob = MetaLearner.forward(phi)\n",
    "            action_probs.append(action_prob)\n",
    "                \n",
    "            action_distribution = Categorical(action_prob)\n",
    "            action = action_distribution.sample()\n",
    "            actions.append(action)\n",
    "\n",
    "            _, reward, _, _ = env.step(action)\n",
    "            rewards[t] = reward\n",
    "        \n",
    "        total_rewards += rewards\n",
    "        slow_RL(optimizer, action_probs, rewards, gamma, action) \n",
    "    \n",
    "    totalrewards = total_rewards/num_tasks\n",
    "    average_rewards = np.mean(total_rewards/num_tasks)\n",
    "   \n",
    "    return average_rewards, totalrewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "25015855-fb1e-4bd2-b23d-318dfd2884bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printresults(total_rewards):\n",
    "    print(\"Avearge Reward {:.2f}\" .format(total_rewards))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "657da623-ece0-4389-a087-6272be7b2e34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m num_tasks \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m      7\u001b[0m num_episodes \u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n\u001b[0;32m----> 8\u001b[0m Avg_Reward, total_rewards \u001b[39m=\u001b[39m testRL2(num_tasks, num_episodes, gamma, env, lr\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m      9\u001b[0m printresults(Avg_Reward)\n",
      "Cell \u001b[0;32mIn[96], line 31\u001b[0m, in \u001b[0;36mtestRL2\u001b[0;34m(num_episodes, num_tasks, gamma, env, lr)\u001b[0m\n\u001b[1;32m     28\u001b[0m         rewards[t] \u001b[39m=\u001b[39m reward\n\u001b[1;32m     30\u001b[0m     total_rewards \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m rewards\n\u001b[0;32m---> 31\u001b[0m     slow_RL(optimizer, action_probs, rewards, gamma, action) \n\u001b[1;32m     33\u001b[0m totalrewards \u001b[39m=\u001b[39m total_rewards\u001b[39m/\u001b[39mnum_tasks\n\u001b[1;32m     34\u001b[0m average_rewards \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(total_rewards\u001b[39m/\u001b[39mnum_tasks)\n",
      "Cell \u001b[0;32mIn[7], line 17\u001b[0m, in \u001b[0;36mslow_RL\u001b[0;34m(optimizer, action_probs, rewards, gamma, action)\u001b[0m\n\u001b[1;32m     15\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m     16\u001b[0m policyLoss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(policyLoss)\u001b[39m.\u001b[39msum()\n\u001b[0;32m---> 17\u001b[0m policyLoss\u001b[39m.\u001b[39;49mbackward(retain_graph\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_bandits = 5\n",
    "# mean = np.random.normal(size=(1,num_bandits))\n",
    "# stddev = np.ones((1,num_bandits))\n",
    "#env = ArmedBanditsEnv(mean, stddev)\n",
    "env = IndependentArms(num_bandits)\n",
    "num_tasks = 1000\n",
    "num_episodes = 100\n",
    "Avg_Reward, total_rewards = testRL2(num_tasks, num_episodes, gamma, env, lr=0)\n",
    "printresults(Avg_Reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "91d60567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB():\n",
    "    def __init__(self, env):\n",
    "        self.c = 0.1\n",
    "        self.env = env\n",
    "        self.num_actions = self.env.num_actions\n",
    "        self.ucb = np.zeros(self.num_actions)\n",
    "        self.action_count = np.zeros(self.num_actions) + 0.0001\n",
    "        self.action_step = 1\n",
    "        self.reward_estimates = np.zeros(self.num_actions)\n",
    "    \n",
    "    def name():\n",
    "        return 'ucb'\n",
    "\n",
    "    def get_action(self):\n",
    "        log_value = np.log(np.full(self.num_actions, self.action_step))\n",
    "        confidence = self.c * np.sqrt(log_value/self.action_count)\n",
    "        action = np.argmax(self.reward_estimates + confidence)\n",
    "        self.action_step += 1\n",
    "        return action\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        _, reward, _, _ = self.env.step(action)\n",
    "        self.action_count[action] += 1\n",
    "        self.reward_estimates[action] += 1/self.action_count[action] *(reward - self.reward_estimates[action])  \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d40ba7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.num_actions = env.num_actions\n",
    "        self.alpha = np.ones(self.num_actions)\n",
    "        self.beta = np.ones(self.num_actions)\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        _, reward, _, _ = self.env.step(action)\n",
    "        self.alpha[action] += reward\n",
    "        self.beta[action] += 1 - reward\n",
    "        \n",
    "        return reward\n",
    "    def name():\n",
    "        return 'thompson'\n",
    "\n",
    "    def get_action(self):\n",
    "        action = np.argmax(np.random.beta(self.alpha, self.beta))\n",
    "        \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0673aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data):\n",
    "    data_x = np.arange(data.size)\n",
    "    p = plt.plot(x, y, o)\n",
    "\n",
    "def plot_data(data, type):\n",
    "\n",
    "    x = np.arange(data[0].size)\n",
    "    for i, y in enumerate(data):\n",
    "        plt.plot(x, y, 'o', markersize = 2, label = type[i])\n",
    "    plt.legend(loc = 'upper right', prop = {'size': 12}, numpoints = 10)\n",
    "    plt.show()\n",
    "\n",
    "def episodic(episodes, trials, num_actions, algo, env):\n",
    "\n",
    "    sum_rewards = np.zeros(trials)\n",
    "    for e in range(episodes):\n",
    "        \n",
    "        algorithm = algo(env)\n",
    "        rewards = np.zeros(trials)\n",
    "\n",
    "        for i in range(trials):\n",
    "            action = algorithm.get_action()\n",
    "            reward = algorithm.get_reward(action)\n",
    "\n",
    "            rewards[i] = reward\n",
    "        sum_rewards += rewards\n",
    "    average_reward = sum_rewards / episodes\n",
    "    \n",
    "    return average_reward\n",
    "\n",
    "def experiments(num_actions, trials, episodes, gamma, lr):\n",
    "\n",
    "    algorithms = [UCB, ThompsonSampling, RNNAgent]\n",
    "    rewards_exp = []\n",
    "    names = []\n",
    "    env = IndependentArms(num_actions)\n",
    "\n",
    "    for algo in algorithms:\n",
    "        if algo == RNNAgent:\n",
    "            _, rewards= testRL2(episodes, trials, gamma, env, lr)\n",
    "            rewards_exp.append(rewards)\n",
    "            names.append(algo.name())\n",
    "            continue\n",
    "        rewards_exp.append(episodic(episodes, trials, num_actions, algo, env))\n",
    "        names.append(algo.name())\n",
    "    \n",
    "    plot_data(rewards_exp, names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "af71228b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (10,) and (1000,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m gamma \u001b[39m=\u001b[39m \u001b[39m0.9\u001b[39m\n\u001b[1;32m      5\u001b[0m lr \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 6\u001b[0m experiments(num_actions, trials, episodes, gamma, lr)\n",
      "Cell \u001b[0;32mIn[101], line 47\u001b[0m, in \u001b[0;36mexperiments\u001b[0;34m(num_actions, trials, episodes, gamma, lr)\u001b[0m\n\u001b[1;32m     44\u001b[0m     rewards_exp\u001b[39m.\u001b[39mappend(episodic(episodes, trials, num_actions, algo, env))\n\u001b[1;32m     45\u001b[0m     names\u001b[39m.\u001b[39mappend(algo\u001b[39m.\u001b[39mname())\n\u001b[0;32m---> 47\u001b[0m plot_data(rewards_exp, names)\n",
      "Cell \u001b[0;32mIn[101], line 9\u001b[0m, in \u001b[0;36mplot_data\u001b[0;34m(data, type)\u001b[0m\n\u001b[1;32m      7\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marange(data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msize)\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m i, y \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(data):\n\u001b[0;32m----> 9\u001b[0m     plt\u001b[39m.\u001b[39;49mplot(x, y, \u001b[39m'\u001b[39;49m\u001b[39mo\u001b[39;49m\u001b[39m'\u001b[39;49m, markersize \u001b[39m=\u001b[39;49m \u001b[39m2\u001b[39;49m, label \u001b[39m=\u001b[39;49m \u001b[39mtype\u001b[39;49m[i])\n\u001b[1;32m     10\u001b[0m plt\u001b[39m.\u001b[39mlegend(loc \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mupper right\u001b[39m\u001b[39m'\u001b[39m, prop \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39msize\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m12\u001b[39m}, numpoints \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m)\n\u001b[1;32m     11\u001b[0m plt\u001b[39m.\u001b[39mshow()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/matplotlib/pyplot.py:2812\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2810\u001b[0m \u001b[39m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[39m.\u001b[39mplot)\n\u001b[1;32m   2811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mplot\u001b[39m(\u001b[39m*\u001b[39margs, scalex\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, scaley\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 2812\u001b[0m     \u001b[39mreturn\u001b[39;00m gca()\u001b[39m.\u001b[39;49mplot(\n\u001b[1;32m   2813\u001b[0m         \u001b[39m*\u001b[39;49margs, scalex\u001b[39m=\u001b[39;49mscalex, scaley\u001b[39m=\u001b[39;49mscaley,\n\u001b[1;32m   2814\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m({\u001b[39m\"\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m\"\u001b[39;49m: data} \u001b[39mif\u001b[39;49;00m data \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m {}), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1688\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1445\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1446\u001b[0m \u001b[39mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1447\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1685\u001b[0m \u001b[39m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1687\u001b[0m kwargs \u001b[39m=\u001b[39m cbook\u001b[39m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[39m.\u001b[39mLine2D)\n\u001b[0;32m-> 1688\u001b[0m lines \u001b[39m=\u001b[39m [\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_lines(\u001b[39m*\u001b[39margs, data\u001b[39m=\u001b[39mdata, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)]\n\u001b[1;32m   1689\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m lines:\n\u001b[1;32m   1690\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/matplotlib/axes/_base.py:311\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     this \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m args[\u001b[39m0\u001b[39m],\n\u001b[1;32m    310\u001b[0m     args \u001b[39m=\u001b[39m args[\u001b[39m1\u001b[39m:]\n\u001b[0;32m--> 311\u001b[0m \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_plot_args(\n\u001b[1;32m    312\u001b[0m     this, kwargs, ambiguous_fmt_datakey\u001b[39m=\u001b[39;49mambiguous_fmt_datakey)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch/lib/python3.11/site-packages/matplotlib/axes/_base.py:504\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes\u001b[39m.\u001b[39myaxis\u001b[39m.\u001b[39mupdate_units(y)\n\u001b[1;32m    503\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m] \u001b[39m!=\u001b[39m y\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]:\n\u001b[0;32m--> 504\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y must have same first dimension, but \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    505\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhave shapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    506\u001b[0m \u001b[39mif\u001b[39;00m x\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39mor\u001b[39;00m y\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m    507\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mx and y can be no greater than 2D, but have \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m                      \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mshapes \u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (10,) and (1000,)"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_actions = 5\n",
    "trials = 1000\n",
    "episodes = 1000\n",
    "gamma = 0.9\n",
    "lr = 0\n",
    "experiments(num_actions, trials, episodes, gamma, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
