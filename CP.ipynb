{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c60a5638",
   "metadata": {},
   "source": [
    "## Implementation of $RL^2$: Fast Reinforcement Learning via Slow Reinforcement Learning\n",
    "The goal of this implementation is to use an RNN agent to be able to learn in itself a Reinforcement Learning algorithm that is able to perfect the exploration and exploitation dilemma of the Multi-Armed Bandit context (Meta-RL). The algorithm is tested in multiple environments and is tested for efficacy with other state of the art agents.\n",
    "\n",
    "1. The implemenation uses a Recurrent Neural Network (RNN) which is a Gated Recurrent Unit (GRU) being fed the last action, reward and timestep.\n",
    "2. The policy is optimized using the basic REINFORCE or Vanilla Policy Gradient.\n",
    "3. Training is being performed for 20,000 tasks(environments) where each task is picked from a random and uniform distribution of means and standard deviations.!\n",
    "\n",
    "Following image shows the visualization of our implementation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca1043",
   "metadata": {},
   "source": [
    "![Meta RL Implementation](Implementation.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4ec03d1-a0fe-4691-adae-ba36f0aece17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from gym import spaces\n",
    "from gym.utils import seeding\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b095bb",
   "metadata": {},
   "source": [
    "#### The classes below are the environments that are used to test the efficacy of our model and the agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56baef4-7df3-406d-a090-a68a36628136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmedBanditsEnv():\n",
    "    def __init__(self, mean, stddev):\n",
    "        assert len(mean.shape) == 2 \n",
    "        assert len(stddev.shape) == 2\n",
    "        self.num_actions = mean.shape[1] \n",
    "        self.num_experiments = mean.shape[0]\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "        \n",
    "    def step(self, action):\n",
    "    \n",
    "        sampled_means = self.mean[np.arange(self.num_experiments),action]\n",
    "        sampled_stddevs = self.stddev[np.arange(self.num_experiments),action]\n",
    "        \n",
    "        reward = np.random.normal(loc=sampled_means, scale=sampled_stddevs, size=(1,self.num_experiments))\n",
    "        \n",
    "        observation, done, info = 0, False, dict()\n",
    "        return observation, reward, done, info\n",
    "    \n",
    "    def reset(self):\n",
    "        return 0\n",
    "    \n",
    "    def reset_env(self):\n",
    "        self.mean = np.random.normal(size = (1, self.num_actions))\n",
    "        self.stddev = np.ones((1, self.num_actions))\n",
    "        \n",
    "    def render(self, mode='human', close=False):\n",
    "        pass\n",
    "    \n",
    "    def _seed(self, seed=None):\n",
    "        self.np_random, seed = seeding.np.random(seed)\n",
    "        return [seed]\n",
    "    \n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "229ef643-1706-4145-b20d-f4b2c8cd37c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArmBanditBernoulli():\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "\n",
    "    def reset_env(self):\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "\n",
    "    def reset(self):\n",
    "        return 0\n",
    "\n",
    "    def step(self, action):\n",
    "        reward = np.random.binomial(n = 1, p = self.probs[action], size=1)[0]\n",
    "        observation, done, info = 0, False, dict()\n",
    "        return observation, reward, done, info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3850d6d-f150-4910-aa4e-55872c45386b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndependentArms():\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "\n",
    "    def reset_env(self):\n",
    "        self.probs = np.random.uniform(low = 0, high = 1, size = self.num_actions)\n",
    "    \n",
    "    def step(self, action):\n",
    "        reward = 1 if random.uniform(0,1) <= self.probs[action] else 0\n",
    "        observation, done, info = 0, False, dict()\n",
    "        return observation, reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77ae23c",
   "metadata": {},
   "source": [
    "#### Following classes define our RNN Agent. The agent is a neural network with the input layer a linear layer with an input size of 3 as it takes as input the last action, last reward and timestep a tuple $<a,r,t>$  and the second layer is a GRU with a hidden size of 48, and 2 hidden layers, the output layer is again a linear layer with an ouput size of the number of bandits or actions to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "385c29cb-f450-4132-9b9e-5968d9bc75db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNAgent(nn.Module):\n",
    "    def __init__(self, env, hiddenSize, layers, outputSize, inputSize = 3, timesteps = 1):\n",
    "        super(RNNAgent, self).__init__()\n",
    "        self.timesteps = timesteps\n",
    "        self.hiddenSize = hiddenSize\n",
    "        self.layers = layers\n",
    "        self.inputSize = inputSize\n",
    "        self.outputSize = outputSize\n",
    "        self.env = env\n",
    "        self.layer1 = nn.Linear(inputSize, hiddenSize)\n",
    "        self.layer2 = nn.GRU(hiddenSize, hiddenSize, layers)\n",
    "        self.layer3 = nn.Linear(hiddenSize, outputSize)\n",
    "\n",
    "        self.hiddenLayerReset()\n",
    "    def name():\n",
    "        return 'RL^2'\n",
    "\n",
    "    def hiddenLayerReset(self):\n",
    "        self.hiddenLayer = self.hidden()\n",
    "    \n",
    "    def hidden(self):\n",
    "        layers = self.layers\n",
    "        hiddenSize = self.hiddenSize\n",
    "        timesteps   = self.timesteps\n",
    "\n",
    "        return torch.randn(layers, timesteps, hiddenSize)\n",
    "    \n",
    "    def get_reward(self, action):\n",
    "         _, reward, _, _ = self.env.step(action)\n",
    "         return reward\n",
    "\n",
    "    def name():\n",
    "        return 'RNN'\n",
    "\n",
    "    def forward(self, phi):\n",
    "\n",
    "        inputs = torch.relu(self.layer1(phi))\n",
    "        outputs, self.hiddenLayer = self.layer2(inputs, self.hiddenLayer)\n",
    "        logits = torch.relu(self.layer3(outputs))\n",
    "        action_probs = F.softmax(logits, dim = 2).view(-1)\n",
    "        \n",
    "        return action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43b794e",
   "metadata": {},
   "source": [
    "#### The input to the RNN is a tuple that of $<s,a,r,t>$ and is represented as $\\phi$ Since for the armed bandit is stateless we'll ignore the state for now and that $\\phi$ is a tuple of the values $<a,r,t>$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a2c43f8-527d-4945-a9c0-fa04d454cb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def createphi(state, action, reward, t):\n",
    "    phi = [action, reward, t]\n",
    "    phi = np.reshape(phi, (1,1,3)).astype(np.float32)\n",
    "    \n",
    "    return torch.tensor(phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4a150c",
   "metadata": {},
   "source": [
    "#### This is our \"slow\" policy optimization algorithm which is a simple REINFORCE policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d02c93c9-826f-4d48-9ea2-1f6d1215eb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_RL(optimizer, action_probs, rewards, gamma, action):\n",
    "    #We are using REINFORCE policy gradient here\n",
    "    Gt = 0\n",
    "    discountedRewards = []\n",
    "    policyLoss = []\n",
    "    \n",
    "    for i in reversed(rewards):\n",
    "        Gt = i + gamma * Gt\n",
    "        discountedRewards.insert(0, Gt)\n",
    "    discountedRewards = torch.tensor(discountedRewards)\n",
    "\n",
    "    for action_prob, r in zip(action_probs, discountedRewards):\n",
    "        policyLoss.append(-action_prob * r)\n",
    "\n",
    "    policyLoss = torch.cat(policyLoss).sum()\n",
    "    optimizer.zero_grad()\n",
    "    policyLoss.backward(retain_graph=True)\n",
    "    optimizer.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9217bae4",
   "metadata": {},
   "source": [
    "#### This function is used to train our agent. The training parameters are as follows:\n",
    "1. Number of tasks 20,000 with each task being sampled from the bandit environment. After each task the environment is reset.\n",
    "2. The trials for each task are limited to 100\n",
    "3. Discount rate is fixed at 0.9\n",
    "4. Learning rate is fixed at 0.005\n",
    "5. Number of bandits are 10\n",
    "\n",
    "Optimizer chosen is an RMSProp optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e55dfa11-ee17-4eff-be91-eb5bcf4c1114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainRL2(num_tasks, num_episodes, gamma, env, lr):\n",
    "\n",
    "    MetaLearner = RNNAgent(env, hiddenSize = 48, outputSize = env.num_actions, layers = 2)\n",
    "    optimizer = optim.RMSprop(MetaLearner.parameters(), lr)\n",
    "    total_rewards = []\n",
    "    \n",
    "    for i in tqdm(range(num_tasks)):\n",
    "        MetaLearner.hiddenLayerReset()\n",
    "        action_probs = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        action = 0\n",
    "        reward = 0\n",
    "        env.reset_env()\n",
    "        for t in range(num_episodes):\n",
    "\n",
    "                phi = createphi(0, action, reward, t)\n",
    "                action_prob = MetaLearner.forward(phi)\n",
    "                action_probs.append(action_prob)\n",
    "                \n",
    "                action_distribution = Categorical(action_prob)\n",
    "                action = action_distribution.sample()\n",
    "                actions.append(action)\n",
    "        \n",
    "                reward = MetaLearner.get_reward(action.item())\n",
    "                rewards.append(reward)\n",
    "            \n",
    "        slow_RL(optimizer, action_probs, rewards, gamma, action.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "12ae04a2-75dd-4fec-b18f-5dfdb9dc9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [10:32<00:00, 31.63it/s]\n"
     ]
    }
   ],
   "source": [
    "num_tasks = 20000\n",
    "num_episodes = 100\n",
    "gamma = 0.9\n",
    "lr = 0.005\n",
    "num_bandits = 10\n",
    "mean = np.random.normal(size=(1,num_bandits))\n",
    "stddev = np.ones((1,num_bandits))\n",
    "#env = ArmedBanditsEnv(mean, stddev)\n",
    "env = IndependentArms(num_bandits)\n",
    "trainRL2(num_tasks, num_episodes, gamma, env, lr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3d66423b",
   "metadata": {},
   "source": [
    "#### To test our agent we are going to use the following helper function, the test is performed on 1000 instances of the bandit problem each lasting for 100 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c4c0ea04-2fae-42f4-a027-9f93d1402cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testRL2(num_tasks, num_episodes, gamma, env, lr):\n",
    "\n",
    "    MetaLearner = RNNAgent(env, hiddenSize = 48, outputSize = env.num_actions, layers = 2)\n",
    "    optimizer = optim.RMSprop(MetaLearner.parameters(), lr)\n",
    "    \n",
    "    total_rewards = np.zeros(num_episodes)\n",
    "    \n",
    "    for i in range(num_tasks):\n",
    "        MetaLearner.hiddenLayerReset()\n",
    "        action_probs = []\n",
    "        actions = []\n",
    "        rewards = np.zeros(num_episodes)\n",
    "        action = 0\n",
    "        reward = 0\n",
    "        \n",
    "        for t in range(num_episodes):\n",
    "\n",
    "            phi = createphi(0, action, reward, t)\n",
    "            action_prob = MetaLearner.forward(phi)\n",
    "            action_probs.append(action_prob)\n",
    "                \n",
    "            action_distribution = Categorical(action_prob)\n",
    "            action = action_distribution.sample()\n",
    "            actions.append(action)\n",
    "\n",
    "            reward = MetaLearner.get_reward(action.item())\n",
    "            rewards[t] = reward\n",
    "        \n",
    "        total_rewards += rewards\n",
    "        slow_RL(optimizer, action_probs, rewards, gamma, action.item()) \n",
    "    \n",
    "    totalrewards = total_rewards/num_tasks\n",
    "    average_rewards = np.mean(total_rewards/num_tasks)\n",
    "   \n",
    "    return average_rewards, totalrewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "25015855-fb1e-4bd2-b23d-318dfd2884bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def printresults(total_rewards):\n",
    "    print(\"Avearge Reward {:.2f}\" .format(total_rewards))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b4c99c61",
   "metadata": {},
   "source": [
    "#### Note that we have set the learning rate to zero, which implies that the learning is fixed and we'll try and test it on the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "657da623-ece0-4389-a087-6272be7b2e34",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avearge Reward 0.51\n",
      "[0.489 0.475 0.519 0.519 0.51  0.505 0.523 0.519 0.491 0.51 ]\n"
     ]
    }
   ],
   "source": [
    "num_bandits = 5\n",
    "# mean = np.random.normal(size=(1,num_bandits))\n",
    "# stddev = np.ones((1,num_bandits))\n",
    "#env = ArmedBanditsEnv(mean, stddev)\n",
    "env = IndependentArms(num_bandits)\n",
    "gamma = 0.9\n",
    "Avg_Reward, total_rewards = testRL2(1000, 10, gamma, env, lr=0)\n",
    "printresults(Avg_Reward)\n",
    "print(total_rewards)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "703ca401",
   "metadata": {},
   "source": [
    "### Here we are implementing different SOTA agents that are given in literature this will be used to test performance of our RNN Agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "91d60567",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCB():\n",
    "    def __init__(self, env):\n",
    "        self.c = 0.1\n",
    "        self.env = env\n",
    "        self.num_actions = self.env.num_actions\n",
    "        self.ucb = np.zeros(self.num_actions)\n",
    "        self.action_count = np.zeros(self.num_actions) + 0.0001\n",
    "        self.action_step = 1\n",
    "        self.reward_estimates = np.zeros(self.num_actions)\n",
    "    \n",
    "    def name():\n",
    "        return 'UCB Agent'\n",
    "\n",
    "    def get_action(self):\n",
    "        log_value = np.log(np.full(self.num_actions, self.action_step))\n",
    "        confidence = self.c * np.sqrt(log_value/self.action_count)\n",
    "        action = np.argmax(self.reward_estimates + confidence)\n",
    "        self.action_step += 1\n",
    "        return action\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        _, reward, _, _ = self.env.step(action)\n",
    "        self.action_count[action] += 1\n",
    "        self.reward_estimates[action] += 1/self.action_count[action] *(reward - self.reward_estimates[action])  \n",
    "        return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d40ba7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThompsonSampling():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.num_actions = env.num_actions\n",
    "        self.alpha = np.ones(self.num_actions)\n",
    "        self.beta = np.ones(self.num_actions)\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        _, reward, _, _ = self.env.step(action)\n",
    "        self.alpha[action] += reward\n",
    "        self.beta[action] += 1 - reward\n",
    "        \n",
    "        return reward\n",
    "    def name():\n",
    "        return 'Thompson Agent'\n",
    "\n",
    "    def get_action(self):\n",
    "        action = np.argmax(np.random.beta(self.alpha, self.beta))\n",
    "        \n",
    "        return action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b64fd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy():\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.num_actions = env.num_actions\n",
    "        self.epsilon = 0.1\n",
    "        self.action_count = np.zeros(self.num_actions) + 0.0001\n",
    "        self.reward_estimates = np.zeros(self.num_actions)\n",
    "    \n",
    "    def name():\n",
    "        return 'Epsilon Greedy Agent'\n",
    "    \n",
    "    def get_action(self):\n",
    "\n",
    "        if np.random.uniform(0,1) > self.epsilon:\n",
    "            action = np.argmax(self.reward_estimates)\n",
    "        else:\n",
    "            action = np.random.randint(0, self.num_actions)\n",
    "\n",
    "        return action\n",
    "\n",
    "    def get_reward(self, action):\n",
    "        _, reward, _, _ = self.env.step(action)\n",
    "        self.action_count[action] += 1\n",
    "        self.reward_estimates[action] += 1/self.action_count[action] *(reward - self.reward_estimates[action])  \n",
    "        return reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ac8fa53",
   "metadata": {},
   "source": [
    "### Following function is used to plot the average reward obtained from different agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0673aa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(data):\n",
    "    data_x = np.arange(data.size)\n",
    "    p = plt.plot(x, y, o)\n",
    "\n",
    "def plot_data(data, type):\n",
    "\n",
    "    x = np.arange(data[0].size)\n",
    "    for i, y in enumerate(data):\n",
    "        plt.plot(y, 'o-', label = type[i])\n",
    "    plt.legend(loc = 'upper right', prop = {'size': 12}, numpoints = 10)\n",
    "    plt.xlabel('Number of Iterations')\n",
    "    plt.ylabel('Average reward during iterations')\n",
    "    plt.title('Comparison of Average Rewards')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "69f30f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def episodic(trials, episodes, num_actions, algo, env):\n",
    "\n",
    "    sum_rewards = np.zeros(episodes)\n",
    "    for e in range(trials):\n",
    "        \n",
    "        algorithm = algo(env)\n",
    "        rewards = np.zeros(episodes)\n",
    "\n",
    "        for i in range(episodes):\n",
    "            action = algorithm.get_action()\n",
    "            reward = algorithm.get_reward(action)\n",
    "\n",
    "            rewards[i] = reward\n",
    "        sum_rewards += rewards\n",
    "    average_reward = sum_rewards / trials\n",
    "    \n",
    "    return average_reward\n",
    "\n",
    "def experiments(num_actions, trials, episodes, gamma, lr):\n",
    "\n",
    "    algorithms = [UCB, ThompsonSampling, EpsilonGreedy, RNNAgent]\n",
    "    rewards_exp = []\n",
    "    names = []\n",
    "    env = IndependentArms(num_actions)\n",
    "\n",
    "    for algo in algorithms:\n",
    "        if algo == RNNAgent:\n",
    "            _, rewards= testRL2(trials, episodes, gamma, env, lr)\n",
    "            rewards_exp.append(rewards)\n",
    "            names.append(algo.name())\n",
    "            continue\n",
    "        rewards_exp.append(episodic(trials, episodes, num_actions, algo, env))\n",
    "        names.append(algo.name())\n",
    "    \n",
    "    plot_data(rewards_exp, names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af71228b",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = 5\n",
    "task_episodes = 500\n",
    "trials = 1000\n",
    "gamma = 0.9\n",
    "lr = 0\n",
    "experiments(num_actions, trials, task_episodes, gamma, lr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
